{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import requre package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import nltk,string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    \n",
    "import nltk\n",
    "def read_reviews(f,end):\n",
    "    rere=[]\n",
    "    for i in range(f,end):\n",
    "        name=i.__str__()\n",
    "        data=pd.read_csv(\"/Users/hankchen/Desktop/semester2/660/final_project/wine bar/\"+name+\".csv\",header=0)\n",
    "        gg=data[\"Description\"].copy()\n",
    "        gg=pre_remove(gg)\n",
    "        for j in gg:\n",
    "            rere.append(j)\n",
    "    return rere\n",
    "\n",
    "def read_label(f,end):\n",
    "    label_senti=[]\n",
    "    for i in range(f,end):\n",
    "        name=i.__str__()\n",
    "        data=pd.read_csv(\"/Users/hankchen/Desktop/semester2/660/final_project/wine bar/\"+name+\".csv\",header=0)\n",
    "        rr=data[\"label\"].copy()\n",
    "        for j in rr:\n",
    "            label_senti.append(j)\n",
    "    return label_senti\n",
    "\n",
    "def pre_remove(reviews):\n",
    "    for i in range(len(reviews)):\n",
    "        #turn the text to lower case\n",
    "        reviews[i]=reviews[i].lower()\n",
    "\n",
    "        #remove emoji in the text\n",
    "        RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "        reviews[i] = RE_EMOJI.sub(r'', reviews[i])\n",
    "        \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into Training data and Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load reviews and labels from the dataset\n",
    "#we manally labelled the sentiment for the first 4 wine bar more than 1000 reviews in total \n",
    "rere=read_reviews(1,6) \n",
    "label_senti=read_label(1,6)\n",
    "#set the maximum number of words and maximum length of the document\n",
    "MAX_NB_WORDS=2000\n",
    "MAX_DOC_LEN=150\n",
    "# convert each document to a list of word index as a sequence\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(rere)\n",
    "sequences = tokenizer.texts_to_sequences(rere)\n",
    "padded_sequences = pad_sequences(sequences,  maxlen=MAX_DOC_LEN,padding='post', truncating='post')\n",
    "#seperate date to traning and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, label_senti ,test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 150, 100)     200100      main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_unigram (Conv1D)           (None, 150, 64)      6464        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bigram (Conv1D)            (None, 149, 64)      12864       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_trigram (Conv1D)           (None, 148, 64)      19264       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool_unigram (MaxPooling1D)     (None, 1, 64)        0           conv_unigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pool_bigram (MaxPooling1D)      (None, 1, 64)        0           conv_bigram[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool_trigram (MaxPooling1D)     (None, 1, 64)        0           conv_trigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flat_unigram (Flatten)          (None, 64)           0           pool_unigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flat_bigram (Flatten)           (None, 64)           0           pool_bigram[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_trigram (Flatten)          (None, 64)           0           pool_trigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concate (Concatenate)           (None, 192)          0           flat_unigram[0][0]               \n",
      "                                                                 flat_bigram[0][0]                \n",
      "                                                                 flat_trigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 192)          0           concate[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 192)          37056       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            193         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 275,941\n",
      "Trainable params: 275,941\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "# The dimension for embedding\n",
    "EMBEDDING_DIM=100\n",
    "\n",
    "# define input layer, where a sentence represented as\n",
    "# 1 dimension array with integers\n",
    "main_input = Input(shape=(MAX_DOC_LEN,),dtype='int32', name='main_input')\n",
    "\n",
    "# define the embedding layer\n",
    "embed_1 = Embedding(input_dim=MAX_NB_WORDS+1,output_dim=EMBEDDING_DIM, input_length=MAX_DOC_LEN,name='embedding')(main_input)\n",
    "# define 1D convolution layer\n",
    "conv1d_1= Conv1D(filters=64, kernel_size=1,name='conv_unigram',activation='relu')(embed_1)\n",
    "# define a 1-dimension MaxPooling \n",
    "pool_1 = MaxPooling1D(MAX_DOC_LEN-1+1, name='pool_unigram')(conv1d_1)\n",
    "\n",
    "flat_1 = Flatten(name='flat_unigram')(pool_1)\n",
    "\n",
    "# filters for bigram\n",
    "conv1d_2= Conv1D(filters=64, kernel_size=2, name='conv_bigram',activation='relu')(embed_1)\n",
    "pool_2 = MaxPooling1D(MAX_DOC_LEN-2+1, name='pool_bigram')(conv1d_2)\n",
    "flat_2 = Flatten(name='flat_bigram')(pool_2)\n",
    "# filters for trigram\n",
    "conv1d_3= Conv1D(filters=64, kernel_size=3, name='conv_trigram',activation='relu')(embed_1)\n",
    "pool_3 = MaxPooling1D(MAX_DOC_LEN-3+1, name='pool_trigram')(conv1d_3)\n",
    "flat_3 = Flatten(name='flat_trigram')(pool_3)\n",
    "\n",
    "# Concatenate flattened output\n",
    "z=Concatenate(name='concate')([flat_1, flat_2, flat_3])\n",
    "\n",
    "# Create a dropout layer\n",
    "drop_1=Dropout(rate=0.3, name='dropout')(z)\n",
    "\n",
    "# Create a dense layer\n",
    "dense_1 = Dense(192, activation='relu', name='dense')(drop_1)\n",
    "# Create the output layer\n",
    "preds = Dense(1, activation='sigmoid', name='output')(dense_1)\n",
    "\n",
    "# create the model with input layer\n",
    "# and the output layer\n",
    "model = Model(inputs=main_input, outputs=preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1312 samples, validate on 563 samples\n",
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHES = 15\n",
    "\n",
    "# Exercise 5.4: Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# fit the model and save fitting history to \"training\"\n",
    "training=model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHES,validation_data=[X_test, y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and plot training history\n",
    "df=pd.DataFrame.from_dict(training.history)\n",
    "df.columns=[\"val_loss\", \"val_acc\", \"train_loss\",\"train_acc\"]\n",
    "df.index.name='epoch'\n",
    "print(df)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,3));\n",
    "df[[\"train_acc\", \"val_acc\"]].plot(ax=axes[0]);\n",
    "df[[\"train_loss\", \"val_loss\"]].plot(ax=axes[1]);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1312 samples, validate on 563 samples\n",
      "Epoch 1/15\n",
      " - 8s - loss: 0.0067 - acc: 0.9992 - val_loss: 0.4481 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.92362, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 2/15\n",
      " - 4s - loss: 0.0044 - acc: 0.9992 - val_loss: 0.4847 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92362\n",
      "Epoch 00002: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "BEST_MODEL_FILEPATH=\"/Users/hankchen/Desktop/semester2/660/final_project/best_model\"\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_acc', verbose=2, save_best_only=True, mode='max')\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "training=model.fit(X_train, y_train,batch_size=BATCH_SIZE, epochs=NUM_EPOCHES, callbacks=[earlyStopping, checkpoint],\n",
    "          validation_data=[X_test, y_test],verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 92.72%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.46      0.50        48\n",
      "           1       0.95      0.97      0.96       515\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       563\n",
      "   macro avg       0.75      0.71      0.73       563\n",
      "weighted avg       0.92      0.92      0.92       563\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0    1\n",
       "row_0         \n",
       "0      22   26\n",
       "1      18  497"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model using the save file\n",
    "model.load_weights(\"/Users/hankchen/Desktop/semester2/660/final_project/best_model\")\n",
    "# predict\n",
    "pred=model.predict(X_test)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "lala=np.where(pred>0.9,1,0)\n",
    "print(classification_report(y_test, lala, target_names=['0','1']))\n",
    "pd.crosstab(pd.Series(y_test), pd.Series(lala[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,precision_recall_curve\n",
    "#calculate AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred,pos_label=1)\n",
    "print(auc(fpr, tpr))\n",
    "#plot ROC curve\n",
    "plt.figure();\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2);\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--');\n",
    "plt.xlabel('False Positive Rate');\n",
    "plt.ylabel('True Positive Rate');\n",
    "plt.title('ROC of CNN Model');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_NB_WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-17c74db9e157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#To predict the sentiment for a certain wine bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_NB_WORDS' is not defined"
     ]
    }
   ],
   "source": [
    "#To predict the sentiment for a certain wine bar \n",
    "pp=read_reviews(16,17)   \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(pp)\n",
    "sequences = tokenizer.texts_to_sequences(pp)\n",
    "padded_sequences2 = pad_sequences(sequences,  maxlen=MAX_DOC_LEN,padding='post', truncating='post')\n",
    "predicted3=model.predict(padded_sequences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic extraction using CNN\n",
    "data=pd.read_csv(\"/Users/hankchen/Desktop/semester2/660/final_project/660 final/Trainning_mutilabeled.csv\",header=0)\n",
    "\n",
    "text=list(data[\"Description\"].copy())\n",
    "text=pre_remove(text)\n",
    "labels=data[[\"Food\",\"Service\",\"Atmosphere\",\"Price\"]].copy().values \n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(text)\n",
    "voc=tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "padded_sequences = pad_sequences(sequences,maxlen=150,padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "              \n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              EMBEDDING_DIM=100, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.3, \\\n",
    "              # dropout rate\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              NUM_DENSE_UNITS=100,\\\n",
    "              # number of units in dense layer\n",
    "              PRETRAINED_WORD_VECTOR=None,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.0):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        # use pretrained word vectors\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        # word vectors can be further tuned\n",
    "                        # set it to False if use static word vectors\n",
    "                        trainable=True,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    \n",
    "    if len(conv_blocks)>1:\n",
    "        z=Concatenate(name='concate')(conv_blocks)\n",
    "    else:\n",
    "        z=conv_blocks[0]\n",
    "        \n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(NUM_DENSE_UNITS, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 494 samples, validate on 212 samples\n",
      "Epoch 1/10\n",
      " - 14s - loss: 0.6362 - acc: 0.7085 - val_loss: 0.5720 - val_acc: 0.7311\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57198, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.5713 - acc: 0.7227 - val_loss: 0.5488 - val_acc: 0.7311\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57198 to 0.54875, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.5506 - acc: 0.7338 - val_loss: 0.5225 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54875 to 0.52255, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.5108 - acc: 0.7794 - val_loss: 0.4904 - val_acc: 0.7854\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52255 to 0.49040, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.4603 - acc: 0.7955 - val_loss: 0.4407 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49040 to 0.44070, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.4057 - acc: 0.8289 - val_loss: 0.3946 - val_acc: 0.8502\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44070 to 0.39465, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.3461 - acc: 0.8679 - val_loss: 0.3622 - val_acc: 0.8679\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.39465 to 0.36224, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.2931 - acc: 0.8912 - val_loss: 0.3351 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.36224 to 0.33514, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.2549 - acc: 0.9049 - val_loss: 0.3219 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33514 to 0.32186, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.2156 - acc: 0.9124 - val_loss: 0.3137 - val_acc: 0.8774\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.32186 to 0.31368, saving model to /Users/hankchen/Desktop/semester2/660/final_project/best_model\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=100\n",
    "FILTER_SIZES=[1,2,3,4]\n",
    "\n",
    "# as the number of classes\n",
    "num_filters=64\n",
    "# set the dense units\n",
    "dense_units_num= num_filters*len(FILTER_SIZES)\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 20\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, labels ,test_size=0.3, random_state=1)\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, MAX_DOC_LEN, NUM_FILTERS=num_filters,NUM_OUTPUT_UNITS=4, NUM_DENSE_UNITS=dense_units_num)\n",
    "\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH, monitor='val_loss',verbose=2, save_best_only=True, mode='min')\n",
    "\n",
    "training=model.fit(X_train, Y_train, batch_size=64, epochs=10,callbacks=[earlyStopping, checkpoint], validation_data=[X_test, Y_test], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#traning history\n",
    "df=pd.DataFrame.from_dict(training.history)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,3));\n",
    "df[[\"val_loss\", \"loss\"]].plot(ax=axes[0]);\n",
    "df[[\"val_acc\", \"acc\"]].plot(ax=axes[1]);\n",
    "plt.show();\n",
    "#predict and classify the output using different theshold\n",
    "pred=model.predict(X_test)\n",
    "pred[:,0]=np.where(pred[:,0]>0.5, 1, 0)\n",
    "pred[:,1]=np.where(pred[:,1]>0.5, 1, 0)\n",
    "pred[:,2]=np.where(pred[:,2]>0.28, 1, 0)\n",
    "pred[:,3]=np.where(pred[:,3]>0.17, 1, 0)\n",
    "#see the performance of this model\n",
    "print(classification_report(Y_test, pred,target_names=['Food','Service','Atmosphere','Price']))\n",
    "#confusion matrix\n",
    "print(pd.crosstab(pred[:,0],Y_test[:,0]))\n",
    "print(pd.crosstab(pred[:,1],Y_test[:,1]))\n",
    "print(pd.crosstab(pred[:,2],Y_test[:,2]))\n",
    "print(pd.crosstab(pred[:,3],Y_test[:,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To predict the topic for a certain wine bar\n",
    "#let's take a 8th bar in our data as an example\n",
    "#pp=read_reviews(14,15)   \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(pp)\n",
    "sequences = tokenizer.texts_to_sequences(pp)\n",
    "padded_sequences3 = pad_sequences(sequences,  maxlen=MAX_DOC_LEN,padding='post', truncating='post')\n",
    "predicted4=model.predict(padded_sequences3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reviews: 178\n",
      "Food: 0.3442622950819672\n",
      "Service: 0.41935483870967744\n",
      "Atmosphere: 0.13333333333333333\n",
      "Price: 0.6111111111111112\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>Service</th>\n",
       "      <th>Atmosphere</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Food  Service  Atmosphere  Price\n",
       "col_0                                  \n",
       "0        42       26           6     44\n",
       "1        80       36          39     28"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Result interpret\n",
    "#topic result classify\n",
    "predicted4[:,0]=np.where(predicted4[:,0]>0.5, 1, 0)\n",
    "predicted4[:,1]=np.where(predicted4[:,1]>0.5, 1, 0)\n",
    "predicted4[:,2]=np.where(predicted4[:,2]>0.28, 1, 0)\n",
    "predicted4[:,3]=np.where(predicted4[:,3]>0.17, 1, 0)\n",
    "predictions2=pd.DataFrame(predicted4,columns=[\"Foodp\",\"Servicep\",\"Atmospherep\",\"Pricep\"])\n",
    "#sentiment result classify\n",
    "predicted3=np.where(predicted3>0.9,1,0)\n",
    "\n",
    "Food=pd.crosstab(predictions2[\"Foodp\"],predicted3[:,0]).iloc[1,:]\n",
    "Service=pd.crosstab(predictions2[\"Servicep\"],predicted3[:,0]).iloc[1,:]\n",
    "Atmosphere=pd.crosstab(predictions2[\"Atmospherep\"],predicted3[:,0]).iloc[1,:]\n",
    "Price=pd.crosstab(predictions2[\"Pricep\"],predicted3[:,0]).iloc[1,:]\n",
    "result=pd.DataFrame(dict(Food=Food, Service=Service,Atmosphere=Atmosphere,Price=Price))\n",
    "print(\"total reviews:\",len(pp))\n",
    "print(\"Food:\",result.iloc[0,0]/(result.iloc[0,0]+result.iloc[1,0]))\n",
    "print(\"Service:\",result.iloc[0,1]/(result.iloc[0,1]+result.iloc[1,1]))\n",
    "print(\"Atmosphere:\",result.iloc[0,2]/(result.iloc[0,2]+result.iloc[1,2]))\n",
    "print(\"Price:\",result.iloc[0,3]/(result.iloc[0,3]+result.iloc[1,3]))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
